#import "../lib.typ":*
#import "@preview/callisto:0.2.4"
#show:doc.with(hw-id: 6)

#let (render, result) = callisto.config(nb: json("hw6.ipynb"))

= 实验要求
本次作业的要求如下：
1. 在上一节课作业的基础上，请利用深度学习方法，对各学科做一个排名模型，能够较好的预测出排名位置，并且利用MSE，MAPE等指标来进行评价模型的优劣。

2. 对ESI的数据进行聚类，发现与华师大类似的学校有哪些，并分析下原因。

= 具体实现
== 准备工作

首先，引入所需的库文件，方便之后的数据分析、数据库导入、可视化等工作。
#render(0)

然后，定义数据库的路径、学校的名称等变量，方便后续使用。
#render(1)
接下来，类似上次作业的代码，定义加载数据库、数据聚合的函数，并且调用。完成数据的加载与预处理工作。
#render(2)


#render(3)
== 深度学习模型构建与训练

- *特征 ($X$) 和 目标 ($y$)*
  - 目标 ($y$)：`rank`

  - 特征 ($X$)：`documents`, `cites`, `cites_per_paper`, `top_papers` (数值特征)，以及 `research_field` (类别特征)。
  - `research_field` 很重要因为 ESI 排名是“在特定学科内”排名的。不同学科的竞争激烈程度和数据分布（如临床医学有几千个机构上榜，而空间科学可能只有几百个）截然不同。因此，模型必须知道它正在为哪个学科进行预测。


- 对数变换：`rank` 值的分布非常偏态（从1到几千）。神经网络在预测这种大跨度值时表现不佳。我们通过 `np.log(y)` 将其压缩，模型去预测 `log(rank)`，最后再用 `np.exp()` 还原回来。


- 数值特征：必须使用 `StandardScaler` 进行标准化。

- 类别特征：必须使用 `OneHotEncoder` 进行独热编码。

#render(4)

我们将构建一个简单的多层感知机。

- *输入层*：维度必须等于处理后的特征数量。

- *隐藏层*：使用 `Dense` 层和 `relu` 激活函数，加入 `Dropout` 防止过拟合。

- *输出层*：`Dense(1)` (因为是回归问题)，使用 `linear` 激活函数（默认）。

#render(5)

我们使用 `fit` 方法训练模型。
`validation_split=0.2`：`Keras` 会自动从训练集中分出 20% 作为验证集，用于在每个 `epoch` 结束时评估模型，帮助我们监控是否过拟合。

#render(6)

模型预测的是 `log(rank)`，我们必须使用 `np.exp()` 将其还原为 `rank`，才能与 `y_test_orig` (原始排名) 进行比较并计算 MSE 和 MAPE。

#render(7)

== 聚类分析：寻找与华师大类似的学校
#v(0.5em)

- 方法1 (聚类)：使用 K-Means 找到 ECNU 所在的“群体”。

- 方法2 (距离)：使用欧氏距离找到 ECNU 的“近邻”或“统计意义上的双胞胎”。

原因是通过比较 ECNU 所在聚类的中心特征与 ECNU 自身的特征，我们可以发现它们在“学科广度”、“平均排名”、“科研产出”等画像上具有相似性，这就是“为什么”它们被分到一类。

我们使用了 `StandardScaler` 进行标准化。`StandardScaler` 通过（均值）和（标准差）来缩放数据，这使它对极端异常值非常敏感。

在高校数据中，像 `total_cites` (总引用) 或 `total_documents` (总论文) 这样的特征是“幂律分布”的——大多数学校产出中等，但少数机构（如中科院、哈佛）的产出是其他学校的几百倍。

这些超级机构的极端数值“拉偏”了 `StandardScaler`，导致它把99%的“普通”大学（包括华师大）全都压缩到了一个很小的数值范围内，使它们看起来都“一样”（都在 Cluster 0），从而聚类失败。

要解决这个问题，我们需要“稳健”的预处理方法。

- 策略一：使用 `RobustScaler` (稳健标准化)
 `RobustScaler` 不使用均值和标准差，而是使用四分位数（IQR） 来缩放数据。这意味着它在计算缩放比例时会忽略掉那些极端的异常值。这是解决当前问题的首选方案。

- 策略二：使用 `np.log1p` (对数变换) 对数变换是处理“幂律分布”数据的经典方法。它可以“压缩”数据的长尾，将 $[100, 1000, 10000000]$ 这样的数据点，变换到更接近的尺度 $[4.6, 6.9, 16.1]$，使数据分布更接近正态分布，K-Means 会更有效。

我们将同时使用这两种策略来构建一个优化的聚类流程。

#render(8)

对高维数据（我们聚类时用了6个特征）进行聚类后，如果能将其“拍扁”到二维平面上展示出来，就能非常直观地看到：

- 不同类别是否在空间上被分开了。

- 华师大位于哪个群体的什么位置。

要实现这一点，最经典的方法是使用 PCA 进行降维。

#render(9)

这张图的结果非常理想，它告诉我们：

- 我们成功地将全球高校分成了几个有意义的类别。

- 华师大属于 `Cluster 0` (紫色)。这个群体是最大的群体，代表了全球“高水平、综合性、规模可观”的主流大学。

其他群体：

- 最右侧的黄色群体 (`Cluster 3`)：这很可能是那些在各项指标上都极端领先的“超级机构”（如中科院、哈佛等）

- 左侧的蓝绿色群体 (`Cluster 1`)：这可能代表了另一类特征的机构，比如“小而精”或者在特定领域（如低 `total_cites` 但高 `avg_rank`）有特点的机构。


#remark[
  完整代码见 `./hw6.ipynb`.
]

